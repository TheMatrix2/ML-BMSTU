# Шпаргалка по машинному обучению для экзамена

## Часть 1. Основы машинного обучения

### 1. Обучение с учителем и без учителя

**Обучение с учителем (Supervised Learning)**
- Обучение происходит на размеченных данных, где есть входные признаки X и известные правильные ответы y
- Цель: построить функцию f(X) → y, которая сможет делать правильные предсказания на новых данных
- Примеры: классификация, регрессия
- Алгоритмы: линейная регрессия, логистическая регрессия, SVM, деревья решений

**Обучение без учителя (Unsupervised Learning)**
- Обучение происходит только на входных данных X без известных ответов
- Цель: найти скрытые закономерности, структуру данных
- Примеры: кластеризация, понижение размерности, поиск аномалий
- Алгоритмы: K-means, иерархическая кластеризация, PCA

### 2. Задача регрессии (с примером)

**Определение**: Предсказание непрерывного числового значения на основе входных признаков.

**Пример**: Предсказание цены квартиры
- Входные признаки: площадь, количество комнат, этаж, район
- Целевая переменная: цена в рублях (непрерывное значение)
- Модель: f(площадь, комнаты, этаж, район) → цена

**Математическая формулировка**:
- Линейная регрессия: y = w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ
- Функция потерь: MSE = 1/n × Σ(yᵢ - ŷᵢ)²

### 3. Задача классификации

**Определение**: Предсказание дискретной метки класса для объекта.

**Типы**:
- **Бинарная классификация**: 2 класса (спам/не спам, больной/здоровый)
- **Многоклассовая**: >2 классов (распознавание цифр 0-9)
- **Многометочная**: объект может принадлежать нескольким классам

**Примеры**:
- Медицинская диагностика: здоров/болен
- Распознавание изображений: кот/собака/птица
- Анализ настроений: позитивный/негативный/нейтральный

**Функции потерь**: кросс-энтропия, hinge loss

### 4. Задача кластеризации

**Определение**: Группировка похожих объектов в кластеры без знания правильных ответов.

**Цель**: Найти скрытую структуру данных, разделить на группы

**Примеры применения**:
- Сегментация клиентов в маркетинге
- Группировка генов в биоинформатике
- Сжатие изображений
- Рекомендательные системы

**Основные алгоритмы**: K-means, иерархическая кластеризация, DBSCAN

## Часть 2. Алгоритмы машинного обучения

### 5. Логистическая регрессия

**Определение**: Алгоритм для задач бинарной и многоклассовой классификации.

**Основная идея**: Использует сигмоидную функцию для преобразования линейной комбинации признаков в вероятность принадлежности к классу.

**Математика**:
- Сигмоида: σ(z) = 1/(1 + e^(-z))
- z = w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ
- P(y=1|x) = σ(z)

**Функция потерь**: Логарифмическая функция потерь (log-loss)

**Преимущества**:
- Интерпретируемость коэффициентов
- Быстрое обучение
- Дает вероятности, а не только предсказания

### 6. SVM (Support Vector Machine)

**Определение**: Алгоритм, который находит оптимальную разделяющую гиперплоскость между классами.

**Основная идея**: Максимизировать отступ (margin) между классами. Опорные векторы - это точки, лежащие на границе отступа.

**Ядра (Kernels)**:
- Линейное: K(xᵢ, xⱼ) = xᵢᵀxⱼ
- Полиномиальное: K(xᵢ, xⱼ) = (xᵢᵀxⱼ + c)^d
- RBF (Гауссово): K(xᵢ, xⱼ) = exp(-γ||xᵢ - xⱼ||²)

**Преимущества**:
- Эффективен в высокоразмерных пространствах
- Использует только опорные векторы (экономия памяти)
- Гибкость благодаря различным ядрам

### 7. Наивный байесовский классификатор

**Определение**: Вероятностный классификатор, основанный на теореме Байеса с предположением о независимости признаков.

**Теорема Байеса**: P(y|x) = P(x|y) × P(y) / P(x)

**Предположение наивности**: Признаки независимы друг от друга:
P(x₁, x₂, ..., xₙ|y) = P(x₁|y) × P(x₂|y) × ... × P(xₙ|y)

**Типы**:
- **Гауссов**: для непрерывных признаков
- **Мультиномиальный**: для счетных данных (текст)
- **Бернулли**: для бинарных признаков

**Применение**: Фильтрация спама, анализ настроений, медицинская диагностика

### 8. Дерево решений

**Определение**: Алгоритм, который строит древовидную модель решений.

**Структура**:
- **Корень**: начальный узел
- **Внутренние узлы**: условия разбиения
- **Листья**: предсказания

**Критерии разбиения**:
- **Энтропия**: H(S) = -Σ pᵢ log₂(pᵢ)
- **Gini impurity**: Gini = 1 - Σ pᵢ²
- **Information Gain**: IG = H(parent) - Σ (|Sᵢ|/|S|) × H(Sᵢ)

**Преимущества**:
- Интерпретируемость
- Работает с категориальными и числовыми признаками
- Не требует предобработки данных

**Недостатки**: Склонность к переобучению

### 9. Случайный лес (Random Forest)

**Определение**: Ансамбль деревьев решений, где каждое дерево обучается на случайной подвыборке данных и признаков.

**Алгоритм**:
1. Создать B деревьев
2. Для каждого дерева:
   - Взять случайную выборку объектов (bootstrap)
   - В каждом узле выбрать случайное подмножество признаков
   - Построить дерево
3. Предсказание: голосование (классификация) или среднее (регрессия)

**Преимущества**:
- Снижает переобучение
- Устойчив к выбросам
- Оценка важности признаков
- Параллелизуемость

**Гиперпараметры**: количество деревьев, максимальная глубина, количество признаков для разбиения

## Часть 3. Оценка качества и предобработка

### 10. Accuracy, Precision, Recall, матрица ошибок

**Матрица ошибок (Confusion Matrix)**:
```
                Предсказано
Реальность    Positive  Negative
Positive        TP       FN
Negative        FP       TN
```

**Метрики**:
- **Accuracy** = (TP + TN) / (TP + TN + FP + FN) - доля правильных предсказаний
- **Precision** = TP / (TP + FP) - доля истинно положительных среди предсказанных положительных
- **Recall** = TP / (TP + FN) - доля найденных положительных объектов
- **F1-score** = 2 × (Precision × Recall) / (Precision + Recall) - гармоническое среднее

**Когда использовать**:
- Accuracy: сбалансированные данные
- Precision: важно избежать ложных положительных (спам-фильтр)
- Recall: важно найти все положительные (медицинская диагностика)

### 11. One Hot Encoding

**Определение**: Метод кодирования категориальных переменных в бинарные векторы.

**Принцип работы**:
Категориальная переменная "Цвет" с значениями [Красный, Зеленый, Синий] превращается в:
- Красный: [1, 0, 0]
- Зеленый: [0, 1, 0]
- Синий: [0, 0, 1]

**Пример**:
```
Исходные данные: [Кот, Собака, Кот, Птица]

После One-Hot:
Кот:    [1, 0, 0]
Собака: [0, 1, 0]
Кот:    [1, 0, 0]
Птица:  [0, 0, 1]
```

**Альтернативы**: Label Encoding, Target Encoding, Binary Encoding

### 12. Cross Validation, валидация на отложенной выборке

**Валидация на отложенной выборке (Hold-out validation)**:
- Данные делятся на тренировочную (70-80%) и тестовую (20-30%) выборки
- Модель обучается на тренировочной, оценивается на тестовой
- Простота реализации, но может быть неустойчивой

**Cross Validation (Кросс-валидация)**:
- **K-fold CV**: данные делятся на k частей, k-1 для обучения, 1 для валидации
- Процедура повторяется k раз, итоговая оценка - среднее
- **Leave-One-Out**: частный случай при k = n (количество объектов)
- **Stratified CV**: сохраняет пропорции классов в каждой части

**Преимущества CV**:
- Более надежная оценка качества
- Использует все данные для обучения и валидации
- Помогает выбрать гиперпараметры

### 13. Устройство нейрона

**Биологическая мотивация**: Искусственный нейрон моделирует работу биологического нейрона.

**Структура**:
1. **Входы**: x₁, x₂, ..., xₙ (сигналы от других нейронов)
2. **Веса**: w₁, w₂, ..., wₙ (сила связей)
3. **Смещение**: b (bias, порог активации)
4. **Функция активации**: f(z), где z = Σwᵢxᵢ + b

**Математическая модель**:
- Взвешенная сумма: z = w₁x₁ + w₂x₂ + ... + wₙxₙ + b
- Выход: y = f(z)

**Процесс работы**:
1. Получить входные сигналы
2. Вычислить взвешенную сумму
3. Применить функцию активации
4. Передать результат дальше

### 14. Градиентный спуск

**Определение**: Итерационный алгоритм оптимизации для поиска минимума функции потерь.

**Основная идея**: Двигаться в направлении, противоположном градиенту (антиградиенту).

**Алгоритм**:
1. Инициализировать параметры θ случайными значениями
2. Вычислить градиент функции потерь: ∇J(θ)
3. Обновить параметры: θ = θ - α × ∇J(θ)
4. Повторять до сходимости

**Типы**:
- **Batch GD**: использует всю выборку для вычисления градиента
- **Stochastic GD**: использует один объект
- **Mini-batch GD**: использует небольшую группу объектов

**Гиперпараметры**:
- **Learning rate (α)**: скорость обучения
- Слишком большой: не сходится
- Слишком маленький: медленная сходимость

## Часть 4. Нейронные сети и глубокое обучение

### 15. Метод обратного распространения ошибки

**Определение**: Алгоритм обучения многослойных нейронных сетей путем распространения ошибки от выходного слоя к входному.

**Алгоритм**:
1. **Forward pass**: вычислить выходы всех слоев
2. **Вычислить ошибку**: L = loss(y_true, y_pred)
3. **Backward pass**: 
   - Вычислить градиенты для выходного слоя
   - Распространить градиенты назад через скрытые слои
   - Использовать правило цепочки: ∂L/∂wᵢⱼ = ∂L/∂aⱼ × ∂aⱼ/∂zⱼ × ∂zⱼ/∂wᵢⱼ
4. **Обновить веса**: w = w - α × ∇w

**Правило цепочки**: Производная сложной функции равна произведению производных составляющих функций.

### 16. Функции активации

**Назначение**: Вносят нелинейность в нейронную сеть, позволяя изучать сложные зависимости.

**Основные функции**:

**Sigmoid**: σ(x) = 1/(1 + e^(-x))
- Выход: (0, 1)
- Проблемы: затухающий градиент, не центрирована относительно нуля

**Tanh**: tanh(x) = (e^x - e^(-x))/(e^x + e^(-x))
- Выход: (-1, 1)
- Лучше sigmoid, но все еще проблема затухающего градиента

**ReLU**: f(x) = max(0, x)
- Простота вычислений
- Решает проблему затухающего градиента
- Проблема: "мертвые" нейроны

**Leaky ReLU**: f(x) = max(αx, x), где α - малое положительное число
- Решает проблему "мертвых" нейронов

**Softmax**: для многоклассовой классификации
- σ(x)ᵢ = e^(xᵢ)/Σe^(xⱼ)
- Выходы интерпретируются как вероятности

### 17. CNN: архитектурные компоненты

**Сверточный слой (Convolutional Layer)**:
- Применяет фильтры (ядра) к входному изображению
- Операция свертки: (I * K)(i,j) = ΣΣ I(m,n) × K(i-m, j-n)
- Выявляет локальные признаки (грани, текстуры)
- Параметры: размер фильтра, количество фильтров, шаг (stride)

**Слой субдискретизации (Pooling Layer)**:
- Уменьшает размерность feature map
- **Max pooling**: берет максимальное значение в окне
- **Average pooling**: берет среднее значение
- Снижает переобучение, уменьшает вычислительную сложность

**Padding**:
- Дополнение входа нулями по краям
- **Valid**: без padding (размер уменьшается)
- **Same**: padding сохраняет размер выхода
- Позволяет контролировать размер выходного тензора

**Flatten**:
- Преобразует многомерный тензор в одномерный вектор
- Переход от сверточных слоев к полносвязным
- Пример: [2,3,4] → [24] вектор

**Полносвязный слой (Dense/Fully Connected)**:
- Каждый нейрон соединен со всеми нейронами предыдущего слоя
- Используется в конце CNN для финальной классификации
- Большое количество параметров

### 18. Dropout

**Определение**: Техника регуляризации, которая случайно "отключает" нейроны во время обучения.

**Принцип работы**:
- Во время обучения: каждый нейрон с вероятностью p обнуляется
- Во время предсказания: все нейроны активны, но их выходы масштабируются

**Преимущества**:
- Предотвращает переобучение
- Заставляет сеть не полагаться на конкретные нейроны
- Эффект ансамблирования

**Типичные значения**: p = 0.2-0.5 для скрытых слоев, p = 0.5-0.8 для входного слоя

### 19. RNN, LSTM

**RNN (Recurrent Neural Network)**:
- Обрабатывает последовательности данных
- Имеет память о предыдущих состояниях
- hₜ = f(Wxₜ + Uhₜ₋₁ + b)
- Проблема: затухающий/взрывающийся градиент

**LSTM (Long Short-Term Memory)**:
- Решает проблему долгосрочных зависимостей в RNN
- Использует систему "ворот" для контроля информационного потока

**Структура LSTM**:
1. **Forget gate**: решает, что забыть из состояния ячейки
2. **Input gate**: решает, какую новую информацию сохранить
3. **Candidate values**: новые значения для добавления
4. **Output gate**: решает, какие части состояния выводить

**Применения**: обработка текста, перевод, анализ временных рядов, генерация последовательностей

## Часть 5. Дополнительные методы и техники

### 20. TF-IDF

**Определение**: Term Frequency-Inverse Document Frequency - метод оценки важности слова в документе относительно коллекции документов.

**Компоненты**:
- **TF (Term Frequency)**: частота термина в документе
  - TF(t,d) = количество вхождений t в d / общее количество слов в d
- **IDF (Inverse Document Frequency)**: обратная частота документа
  - IDF(t,D) = log(|D| / |{d ∈ D : t ∈ d}|)

**Формула**: TF-IDF(t,d,D) = TF(t,d) × IDF(t,D)

**Интуиция**: 
- Часто встречающиеся в документе слова важны (высокий TF)
- Редкие в коллекции слова более информативны (высокий IDF)
- Общие слова (и, в, на) получают низкий вес

**Применение**: поиск информации, классификация текстов, рекомендательные системы

### 21. K-Means

**Определение**: Алгоритм кластеризации, который разбивает данные на k кластеров.

**Алгоритм**:
1. Выбрать количество кластеров k
2. Инициализировать k центроидов случайно
3. Повторять до сходимости:
   - Назначить каждую точку ближайшему центроиду
   - Пересчитать центроиды как среднее точек кластера

**Функция потерь**: Within-Cluster Sum of Squares (WCSS)
- WCSS = Σᵢ₌₁ᵏ Σₓ∈Cᵢ ||x - μᵢ||²

**Выбор k**: метод локтя (elbow method), силуэтный анализ

**Преимущества**: простота, быстрота, хорошо работает на сферических кластерах

**Недостатки**: нужно задавать k, чувствителен к выбросам, предполагает сферические кластеры

### 22. Иерархическая кластеризация

**Определение**: Создает иерархию кластеров в виде дерева (дендрограммы).

**Типы**:
- **Агломеративная**: снизу вверх (объединяет кластеры)
- **Дивизивная**: сверху вниз (разделяет кластеры)

**Алгоритм агломеративной кластеризации**:
1. Каждая точка - отдельный кластер
2. Повторять пока не останется один кластер:
   - Найти два ближайших кластера
   - Объединить их
   - Обновить матрицу расстояний

**Методы связывания**:
- **Single linkage**: расстояние между ближайшими точками
- **Complete linkage**: расстояние между дальними точками
- **Average linkage**: среднее расстояние между всеми парами
- **Ward linkage**: минимизирует внутрикластерную дисперсию

**Преимущества**: не нужно задавать количество кластеров, дендрограмма показывает структуру

### 23. DBSCAN

**Определение**: Density-Based Spatial Clustering - алгоритм кластеризации, основанный на плотности.

**Основные понятия**:
- **ε (epsilon)**: радиус окрестности
- **MinPts**: минимальное количество точек в окрестности
- **Core point**: точка с ≥MinPts соседями в радиусе ε
- **Border point**: не core, но в окрестности core point
- **Noise point**: не core и не border

**Алгоритм**:
1. Для каждой точки определить тип (core/border/noise)
2. Создать кластеры из связанных core points
3. Добавить border points к ближайшим кластерам
4. Noise points остаются без кластера

**Преимущества**:
- Автоматически определяет количество кластеров
- Находит кластеры произвольной формы
- Устойчив к выбросам

**Недостатки**: чувствителен к параметрам ε и MinPts

### 24. Word2vec, embedding слова

**Определение**: Методы представления слов в виде плотных векторов фиксированной размерности.

**Word2vec архитектуры**:
- **CBOW (Continuous Bag of Words)**: предсказывает слово по контексту
- **Skip-gram**: предсказывает контекст по слову

**Принцип**: слова с похожими контекстами имеют похожие векторные представления

**Свойства embeddings**:
- Семантическая близость: king - man + woman ≈ queen
- Арифметические операции над векторами слов
- Размерность обычно 100-300

**Другие методы**:
- **GloVe**: Global Vectors for Word Representation
- **FastText**: учитывает субсловную информацию
- **BERT**: контекстуальные embeddings

**Применения**: машинный перевод, анализ настроений, вопросно-ответные системы

### 25. PCA (Principal Component Analysis)

**Определение**: Метод понижения размерности путем проекции данных на направления максимальной дисперсии.

**Основная идея**: найти новые оси (главные компоненты), вдоль которых данные имеют максимальную вариативность.

**Алгоритм**:
1. Центрировать данные (вычесть среднее)
2. Вычислить ковариационную матрицу
3. Найти собственные векторы и значения
4. Отсортировать по убыванию собственных значений
5. Выбрать k первых компонент
6. Спроецировать данные на новое пространство

**Математика**:
- Ковариационная матрица: C = (1/n) × XᵀX
- Собственные векторы определяют направления главных компонент
- Собственные значения показывают объясненную дисперсию

**Применения**: визуализация, сжатие данных, предобработка перед ML

### 26. t-SNE

**Определение**: t-Distributed Stochastic Neighbor Embedding - метод нелинейного понижения размерности для визуализации.

**Основная идея**: сохранить локальную структуру данных при проекции в низкоразмерное пространство (обычно 2D или 3D).

**Алгоритм**:
1. Вычислить вероятности схожести в исходном пространстве:
   - p_{ij} ∝ exp(-||xᵢ - xⱼ||² / 2σᵢ²)
2. Инициализировать точки в низкоразмерном пространстве
3. Вычислить вероятности схожести в новом пространстве:
   - q_{ij} ∝ (1 + ||yᵢ - yⱼ||²)^(-1)
4. Минимизировать KL-дивергенцию между p и q

**Параметры**:
- **Perplexity**: определяет количество ближайших соседей (обычно 5-50)
- **Learning rate**: скорость оптимизации
- **Количество итераций**: обычно 1000+

**Особенности**:
- Отлично для визуализации кластеров
- Сохраняет локальную, но не глобальную структуру
- Расстояния между кластерами не интерпретируемы
- Вычислительно сложен O(n²)

**Отличия от PCA**:
- PCA: линейный, сохраняет глобальную структуру
- t-SNE: нелинейный, сохраняет локальную структуру, лучше для визуализации

---

*Эта шпаргалка покрывает основные концепции машинного обучения. Для углубленного понимания рекомендуется изучить математические выводы и реализовать алгоритмы на практике.*